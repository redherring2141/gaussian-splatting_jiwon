"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description"
"0","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(3322, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","225,956",
"0","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(3322, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","7,230,592",
"0","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(3322, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","6,805,056",
"1","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(19932, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","6,856,386",
"1","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(19932, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","219,404,352",
"1","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(19932, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","204,097,152",
"2","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","4,936",
"2","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","157,952",
"2","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","135,824",
"2","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedSharedAccess","WRN","This kernel has uncoalesced shared accesses resulting in a total of 128 excessive wavefronts (17% of the total 752 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa) has an example on optimizing shared memory accesses."
"3","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","48",
"3","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,408",
"3","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,392",
"4","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","67",
"4","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,720",
"4","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,588",
"4","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 1 excessive sectors (50% of the total 2 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"5","79495","python3.11","127.0.0.1","xxtrf4_set_info_ker(int, int *)","1","7","(1, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","7",
"5","79495","python3.11","127.0.0.1","xxtrf4_set_info_ker(int, int *)","1","7","(1, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","7",
"5","79495","python3.11","127.0.0.1","xxtrf4_set_info_ker(int, int *)","1","7","(1, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","7",
"6","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","1,023",
"6","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","21,366",
"6","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","17,664",
"7","79495","python3.11","127.0.0.1","void ipiv_lower_small<float, (int)32>(int, void *, int, long *, int, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","1,220",
"7","79495","python3.11","127.0.0.1","void ipiv_lower_small<float, (int)32>(int, void *, int, long *, int, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","38,144",
"7","79495","python3.11","127.0.0.1","void ipiv_lower_small<float, (int)32>(int, void *, int, long *, int, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","33,104",
"7","79495","python3.11","127.0.0.1","void ipiv_lower_small<float, (int)32>(int, void *, int, long *, int, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 4 excessive sectors (20% of the total 20 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"8","79495","python3.11","127.0.0.1","void create_pivot_v2<(int)32>(int, int *, long *, int)","1","7","(32, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","220",
"8","79495","python3.11","127.0.0.1","void create_pivot_v2<(int)32>(int, int *, long *, int)","1","7","(32, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","7,040",
"8","79495","python3.11","127.0.0.1","void create_pivot_v2<(int)32>(int, int *, long *, int)","1","7","(32, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","6,928",
"8","79495","python3.11","127.0.0.1","void create_pivot_v2<(int)32>(int, int *, long *, int)","1","7","(32, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 9 excessive sectors (50% of the total 18 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"9","79495","python3.11","127.0.0.1","void ipiv_lower_diag<float, (int)32>(int, void *, int, int *, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","36",
"9","79495","python3.11","127.0.0.1","void ipiv_lower_diag<float, (int)32>(int, void *, int, int *, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,152",
"9","79495","python3.11","127.0.0.1","void ipiv_lower_diag<float, (int)32>(int, void *, int, int *, int)","1","7","(32, 1, 1)","(4, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,152",
"10","79495","python3.11","127.0.0.1","void ipiv_64_to_32_ker<(int)128>(long, const long *, int *)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","40",
"10","79495","python3.11","127.0.0.1","void ipiv_64_to_32_ker<(int)128>(long, const long *, int *)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,056",
"10","79495","python3.11","127.0.0.1","void ipiv_64_to_32_ker<(int)128>(long, const long *, int *)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,052",
"10","79495","python3.11","127.0.0.1","void ipiv_64_to_32_ker<(int)128>(long, const long *, int *)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 1 excessive sectors (33% of the total 3 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"11","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","23",
"11","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","395",
"11","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","394",
"11","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","PCSamplingData","WRN","Sampling metrics were enabled, but no samples could be collected for this kernel."
"12","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","23",
"12","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","395",
"12","79495","python3.11","127.0.0.1","void <unnamed>::elementwise_kernel_with_index<int, at::native::arange_cuda_out(const c10::Scalar &, const c10::Scalar &, const c10::Scalar &, at::Tensor &)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 4)]::operator ()() const::[lambda(long) (instance 1)]>(T1, T2, function_traits<T2>::result_type *)","1","7","(64, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","394",
"13","79495","python3.11","127.0.0.1","void laswp_kernel<float, (bool)0>(int, T1 *const *, int, int, int, const int *, int, int, int)","1","7","(4, 64, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","188",
"13","79495","python3.11","127.0.0.1","void laswp_kernel<float, (bool)0>(int, T1 *const *, int, int, int, const int *, int, int, int)","1","7","(4, 64, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","2,096",
"13","79495","python3.11","127.0.0.1","void laswp_kernel<float, (bool)0>(int, T1 *const *, int, int, int, const int *, int, int, int)","1","7","(4, 64, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","2,060",
"13","79495","python3.11","127.0.0.1","void laswp_kernel<float, (bool)0>(int, T1 *const *, int, int, int, const int *, int, int, int)","1","7","(4, 64, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 12 excessive sectors (41% of the total 29 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"14","79495","python3.11","127.0.0.1","void trsm_batch_left_lower_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","123",
"14","79495","python3.11","127.0.0.1","void trsm_batch_left_lower_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","660",
"14","79495","python3.11","127.0.0.1","void trsm_batch_left_lower_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","616",
"14","79495","python3.11","127.0.0.1","void trsm_batch_left_lower_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 8 excessive sectors (36% of the total 22 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"15","79495","python3.11","127.0.0.1","void trsm_batch_left_upper_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","291",
"15","79495","python3.11","127.0.0.1","void trsm_batch_left_upper_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,173",
"15","79495","python3.11","127.0.0.1","void trsm_batch_left_upper_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,007",
"15","79495","python3.11","127.0.0.1","void trsm_batch_left_upper_kernel_small<float, (int)4, (int)4, (int)64>(cublasTrsmBatchParams<T1>, const T1 *const *, T1 *const *, const T1 *, T1)","1","7","(4, 8, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 8 excessive sectors (36% of the total 22 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"16","79495","python3.11","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 11)]::operator ()() const::[lambda(bool) (instance 1)], at::detail::Array<char *, (int)2>, TrivialOffsetCalculator<(int)1, unsigned int>, TrivialOffsetCalculator<(int)1, unsigned int>, at::native::memory::LoadWithCast<(int)1>, at::native::memory::StoreWithCast<(int)1>>(int, T1, T2, T3, T4, T5, T6)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","110",
"16","79495","python3.11","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 11)]::operator ()() const::[lambda(bool) (instance 1)], at::detail::Array<char *, (int)2>, TrivialOffsetCalculator<(int)1, unsigned int>, TrivialOffsetCalculator<(int)1, unsigned int>, at::native::memory::LoadWithCast<(int)1>, at::native::memory::StoreWithCast<(int)1>>(int, T1, T2, T3, T4, T5, T6)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","2,093",
"16","79495","python3.11","127.0.0.1","void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIteratorBase &)::[lambda() (instance 2)]::operator ()() const::[lambda() (instance 11)]::operator ()() const::[lambda(bool) (instance 1)], at::detail::Array<char *, (int)2>, TrivialOffsetCalculator<(int)1, unsigned int>, TrivialOffsetCalculator<(int)1, unsigned int>, at::native::memory::LoadWithCast<(int)1>, at::native::memory::StoreWithCast<(int)1>>(int, T1, T2, T3, T4, T5, T6)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","2,084",
"17","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(3322, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","225,956",
"17","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(3322, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","7,230,592",
"17","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(3322, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","6,805,056",
"18","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(19932, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","6,856,386",
"18","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(19932, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","219,404,352",
"18","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::BinaryFunctor<float, float, float, at::native::binary_internal::MulFunctor<float>>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(19932, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","204,097,152",
"19","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","4,936",
"19","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","157,952",
"19","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","135,824",
"19","79495","python3.11","127.0.0.1","ampere_sgemm_32x128_tt","1","7","(256, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedSharedAccess","WRN","This kernel has uncoalesced shared accesses resulting in a total of 128 excessive wavefronts (17% of the total 752 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa) has an example on optimizing shared memory accesses."
"20","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","48",
"20","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,408",
"20","79495","python3.11","127.0.0.1","void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::Array<char *, (int)1>>(int, T2, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,392",
"21","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","67",
"21","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","1,720",
"21","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","1,588",
"21","79495","python3.11","127.0.0.1","void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl<at::native::FillFunctor<float>>(at::TensorIteratorBase &, const T1 &)::[lambda(int) (instance 1)]>(int, T3)","1","7","(128, 1, 1)","(1, 1, 1)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","WRN","This kernel has uncoalesced global accesses resulting in a total of 1 excessive sectors (50% of the total 2 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) had additional information on reducing uncoalesced device memory accesses."
"22","79495","python3.11","127.0.0.1","xxtrf4_set_info_ker(int, int *)","1","7","(1, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","inst_executed","inst","7",
"22","79495","python3.11","127.0.0.1","xxtrf4_set_info_ker(int, int *)","1","7","(1, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed","inst","7",
"22","79495","python3.11","127.0.0.1","xxtrf4_set_info_ker(int, int *)","1","7","(1, 1, 1)","(1, 1, 1)","0","8.0","Command line profiler metrics","thread_inst_executed_true","inst","7",
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","Command line profiler metrics","inst_executed","","n/a",
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","Command line profiler metrics","thread_inst_executed","","n/a",
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","Command line profiler metrics","thread_inst_executed_true","","n/a",
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","SourceCounters","","","","PCSamplingData","ERR","Rule PC sampling data returned an error: Metric smsp__pcsamp_sample_count not found"
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","SourceCounters","","","","PCSamplingData","ERR","<built-in function IAction_metric_by_name> returned a result with an exception set
/home/jiwon/Documents/NVIDIA Nsight Compute/2023.1.1/Sections/PCSamplingData.py:47
/opt/nvidia/nsight-compute/2023.1.1/target/linux-desktop-glibc_2_11_3-x64/../../sections/NvRules.py:2062"
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","ERR","Rule Uncoalesced Global Accesses returned an error: Metric memory_l2_theoretical_sectors_global not found"
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","ERR","<built-in function IAction_metric_by_name> returned a result with an exception set
/home/jiwon/Documents/NVIDIA Nsight Compute/2023.1.1/Sections/UncoalescedAccess.py:46
/opt/nvidia/nsight-compute/2023.1.1/target/linux-desktop-glibc_2_11_3-x64/../../sections/NvRules.py:2062"
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","SourceCounters","","","","UncoalescedSharedAccess","ERR","Rule Uncoalesced Shared Accesses returned an error: Metric memory_l1_wavefronts_shared not found"
"23","79495","python3.11","127.0.0.1","void getrf_pivot<getrf_params_<float, (int)32, (int)1, (int)32, (int)32, (int)1>>(int, int, int, void *, int, long *, int, T1::data_type *, unsigned int *, unsigned int *, T1::data_type *, unsigned int, unsigned int, unsigned int, int *)","1","7","(0, 0, 0)","(0, 0, 0)","0","8.0","SourceCounters","","","","UncoalescedSharedAccess","ERR","<built-in function IAction_metric_by_name> returned a result with an exception set
/home/jiwon/Documents/NVIDIA Nsight Compute/2023.1.1/Sections/UncoalescedSharedAccess.py:46
/opt/nvidia/nsight-compute/2023.1.1/target/linux-desktop-glibc_2_11_3-x64/../../sections/NvRules.py:2062"
